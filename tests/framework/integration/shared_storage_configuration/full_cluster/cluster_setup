#!/bin/bash -ex

spacelist_to_commalist() {
    echo $@ | tr ' ' ','
}

[ -r localenv ] && . localenv

CLUSTER_CONFIG=${CLUSTER_CONFIG:-"$(ls $PWD/shared_storage_configuration_cluster_cfg.json)"}

. $CHROMA_DIR/tests/framework/utils/cluster_setup.sh

# need to remove the chroma repositories configured by the provisioner
pdsh -l root -R ssh -S -w $(spacelist_to_commalist $CHROMA_MANAGER ${STORAGE_APPLIANCES[@]} ${WORKERS[@]}) "exec 2>&1; set -xe
# Clean out any yum info, if this is a manual system not automated the underlying repos might have changed.
yum clean all

$PROXY yum install -y omping
if $MEASURE_COVERAGE && [ -f /etc/yum.repos.d/autotest.repo ]; then
    cat << \"EOF\" >> /etc/yum.repos.d/autotest.repo
retries=50
timeout=180
EOF
    $PROXY yum install -y python-setuptools python2-coverage
fi
if [ -f /etc/yum.repos.d/autotest.repo ]; then
    rm -f /etc/yum.repos.d/autotest.repo
fi" | dshbak -c
if [ ${PIPESTATUS[0]} != 0 ]; then
    exit 1
fi

# Install and setup manager on integration test runner
if $JENKINS; then
    ARCHIVE_PATH=.
else
    ARCHIVE_PATH=$CHROMA_DIR/_topdir/RPMS/noarch/
fi

if [ -f ~/storage_server.repo.in ]; then
    STORAGE_SERVER_REPO=~/storage_server.repo.in
else
    STORAGE_SERVER_REPO=$CHROMA_DIR/storage_server.repo
fi

if [ -n "$RELEASE_ARCHIVE" ]; then
    ARCHIVE_NAME=${RELEASE_ARCHIVE##*/}
fi

# Install and setup manager
scp $CHROMA_DIR/chroma_support.repo $STORAGE_SERVER_REPO $(ls $ARCHIVE_PATH/$ARCHIVE_NAME) $CHROMA_DIR/tests/utils/install.exp root@$CHROMA_MANAGER:/tmp
ssh root@$CHROMA_MANAGER "#don't do this, it hangs the ssh up, when used with expect, for some reason: exec 2>&1
set -ex

mv /tmp/chroma_support.repo /etc/yum.repos.d/

# install cman here to test that the fence-agents-iml package is being a
# 'duck-like' replacement for fence -agents since cman depends on
# fence-agents
yum -y install expect cman
# Install from the installation package
cd /tmp
yum -y install \$(ls $ARCHIVE_NAME)
expect install.exp $CHROMA_USER $CHROMA_EMAIL $CHROMA_PASS ${CHROMA_NTP_SERVER:-localhost}
#chroma-config bundle register /var/lib/chroma/repo/external/7
#for profile in \$(ls /usr/share/chroma-manager/*.profile); do
#    echo \"Registering profile: \$profile\"
#    chroma-config profile register \"\$profile\"
#done 

# override /usr/share/chroma-manager/storage_server.repo
if [ -f /tmp/storage_server.repo.in ]; then
    # make sure we use the correct lustre though!
    sed -e \"s/@LUSTRE_SERVER_URL@/${LUSTRE_SERVER_URL//\//\\\\/}/\" \
        -e \"s/@LUSTRE_CLIENT_URL@/${LUSTRE_CLIENT_URL//\//\\\\/}/\" \
        -e 's/manager-for-lustre/manager-for-lustre-devel/g' \
        < /tmp/storage_server.repo.in > /usr/share/chroma-manager/storage_server.repo
fi
# add any repos needed by the test
if [ -n \"$STORAGE_SERVER_REPOS\" ]; then
    for repo in $STORAGE_SERVER_REPOS; do
        {
            echo
            curl \"\$repo\"
        } >> /usr/share/chroma-manager/storage_server.repo
    done
fi

cat <<\"EOF1\" > /usr/share/chroma-manager/local_settings.py
import logging
LOG_LEVEL = logging.DEBUG
$LOCAL_SETTINGS
EOF1

# https://github.com/pypa/virtualenv/issues/355
python_version=\$(python -c 'import platform; print \".\".join(platform.python_version_tuple()[0:2])')
if $MEASURE_COVERAGE; then
    cat <<\"EOF1\" > /usr/share/chroma-manager/.coveragerc
[run]
data_file = /var/tmp/.coverage
parallel = True
source = /usr/share/chroma-manager/
EOF1
    cat <<\"EOF1\" > /usr/lib/python\$python_version/site-packages/sitecustomize.py
import coverage
cov = coverage.coverage(config_file='/usr/share/chroma-manager/.coveragerc', auto_data=True)
cov.start()
cov._warn_no_data = False
cov._warn_unimported_source = False
EOF1
else
    # Ensure that coverage is disabled
    rm -f /usr/lib/python\$python_version/site-packages/sitecustomize.py*
fi"


# Install and setup chroma software storage appliances
# shellcheck disable=SC2153
pdsh -l root -R ssh -S -w $(spacelist_to_commalist ${STORAGE_APPLIANCES[@]} ${WORKERS[@]}) "exec 2>&1; set -xe
# https://github.com/pypa/virtualenv/issues/355
python_version=\$(python -c 'import platform; print \".\".join(platform.python_version_tuple()[0:2])')
if $MEASURE_COVERAGE; then
    cat <<\"EOF\" > /usr/lib/python\$python_version/site-packages/.coveragerc
[run]
data_file = /var/tmp/.coverage
parallel = True
source = /usr/lib/python\$python_version/site-packages/chroma_agent/
EOF
    cat <<\"EOF\" > /usr/lib/python\$python_version/site-packages/sitecustomize.py
import coverage
cov = coverage.coverage(config_file='/usr/lib/python\$python_version/site-packages/.coveragerc', auto_data=True)
cov.start()
cov._warn_no_data = False
cov._warn_unimported_source = False
EOF
else
    # Ensure that coverage is disabled
    rm -f /usr/lib/python\$python_version/site-packages/sitecustomize.py*
fi

if $USE_FENCE_XVM; then
    # fence_xvm support
    mkdir -p /etc/cluster
    echo \"not secure\" > /etc/cluster/fence_xvm.key
fi" | dshbak -c
if [ ${PIPESTATUS[0]} != 0 ]; then
    exit 1
fi

source $CHROMA_DIR/tests/framework/integration/utils/install_client.sh

# Install and setup integration tests on integration test runner
if ! $JENKINS; then
    CMIT=$(ls $CHROMA_DIR/_topdir/RPMS/noarch/python2-iml-manager-integration-tests-*.noarch.rpm)
fi
scp $CMIT $CLUSTER_CONFIG root@$TEST_RUNNER:/root/
ssh root@$TEST_RUNNER <<EOF
exec 2>&1; set -xe
if $JENKINS; then
    $PROXY yum --disablerepo=\* --enablerepo=chroma makecache
    CMIT=chroma-manager-integration-tests
else
    CMIT=/root/${CMIT##*/}
fi

# set up required repos
yum-config-manager --add-repo https://copr.fedorainfracloud.org/coprs/$COPR_OWNER/$COPR_PROJECT/repo/epel-7/$COPR_OWNER-$COPR_PROJECT-epel-7.repo
# add any repos required by this test run
if [ -n \"$TEST_RUNNER_REPOS\" ]; then
    for repo in $TEST_RUNNER_REPOS; do
        yum-config-manager --add-repo \$repo
    done
fi
yum -y install epel-release

if ! $PROXY yum -y install \$CMIT python-mock; then
    $PROXY yum clean all
    $PROXY yum -y install \$CMIT python-mock
fi

# Set up fencing on the vm host
if $USE_FENCE_XVM; then
    # make sure the host has fence_virtd installed and configured
    ssh root@$HOST_IP "exec 2>&1; set -xe
    uname -a
    $PROXY yum install -y fence-virt fence-virtd fence-virtd-libvirt fence-virtd-multicast
    mkdir -p /etc/cluster
    echo \"not secure\" > /etc/cluster/fence_xvm.key
    restorecon -Rv /etc/cluster/
    cat <<\"EOF1\" > /etc/fence_virt.conf
backends {
	libvirt {
		uri = \"qemu:///system\";
	}

}

listeners {
	multicast {
		port = \"1229\";
		family = \"ipv4\";
		address = \"225.0.0.12\";
		key_file = \"/etc/cluster/fence_xvm.key\";
		interface = \"virbr0\";
	}

}

fence_virtd {
	module_path = \"/usr/lib64/fence-virt\";
	backend = \"libvirt\";
	listener = \"multicast\";
}
EOF1
    chkconfig --add fence_virtd
    chkconfig fence_virtd on
    service fence_virtd restart"
fi
EOF

$CHROMA_DIR/tests/framework/integration/utils/wait_for_nodes.sh "$CLIENT_1"

echo "End installation and setup."
